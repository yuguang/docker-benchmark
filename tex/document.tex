\documentclass[11pt]{article}
\usepackage{longtable}
\usepackage{listings}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage[margin=1in]{geometry}
\usepackage[colorlinks]{hyperref}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{enumerate}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{shapes}

\newcommand{\C}{{\mathbb{C}}}
\newcommand{\F}{{\mathbb{F}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\e}[1]{\ensuremath{\times 10^{#1}}}

\newcommand{\ve}[1]{\boldsymbol{#1}}
\newcommand{\norm}[1]{\|{#1}\|}
\newcommand{\code}{\begingroup
	\catcode`_=12 \docode}
\newcommand{\docode}[2]{
	\begin{framed}
		\lstinputlisting[basicstyle=\ttfamily\scriptsize,language=#2,title=\underline{\texttt{#1}},tabsize=4,numbers=left]{#1}\end{framed}\endgroup}


\setlength\parindent{0pt}
\setlength{\parskip}{3mm plus3mm minus2mm}

\lstset{basicstyle=\ttfamily,showstringspaces=false}
\lstset{language=C++,
	basicstyle=\ttfamily,
	keywordstyle=\color{blue}\ttfamily,
	stringstyle=\color{red}\ttfamily,
	commentstyle=\color{gray}\ttfamily}

\begin{document}
	
	\thispagestyle{empty}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\title{Performance Evaluation of Docker}
	
	\author{Yuguang Zhang \\ University of Waterloo}
	
	\maketitle
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
Virtualization of operating systems provides a common way to deploy complex
applications to a cloud environment. A new form of virtualization based on containers
promises to offer better performance than traditional approaches with a virtual machine.
This paper explores a method of running performance evaluation experiments using
docker and compares container based virtualization with native performance. Several
benchmark tools are used to measure overhead in terms of processing, storage,
memory and network. [Summary of results]
\end{abstract}

\section{Introduction}
Container-based virtualization is a lightweight alternative to hypervisor-based virtualization \cite{morabito}. Docker, a form of container-based virtualization, uses Linux kernel primitives to implement isolation of processes, network interfaces, and file systems. Creating a new container takes less than a second compared to minutes for spinning up a new virtual machine. This is because virtual machines run on virtualized hardware and virtual device drivers which presents performance challenges due to the added layers between the physical hardware and the application. This paper explores Docker as an alternative to the reboot per job structure for running performance evaluation experiments in DataMill. 
DataMill is an infrastructure for rigorous and repeatable performance evaluation \cite{augusto}. DataMill executes the experiments on real hardware and incorporates the results from existing research on how to design experiments and evaluate hidden factors. The infrastructure automatically varies a selection of hardware and software factors, and therefore reduces the effort required by the user to setup and configure the experiment while simultaneously increasing the robustness and applicability of the experiments results to a wide range of factor levels \cite{augusto}. In order to execute experiments on real hardware and give users a clean workspace for experiments, DataMill currently makes a copy of the root filesystem to run benchmarks. DataMill workers consist of three partitions: boot, benchmark, and controller. The boot partition holds the kernel and grub. The controller partition is responsible for communication with the master node. The benchmark partition is where experiments are run. To run benchmarks, DataMill workers reboots into the benchmark partition. After running benchmarks, workers reboot to the controller partition. 
Docker is an alternative to the current dual boot structure while maintaining an isolated environment for each experiment. This paper explores the overhead of running benchmarks in Docker. The goal is to quantify the level of overhead introduced by Docker. The native performance is used as the reference for comparison. The main contributions of this paper are:
\begin{itemize}
	\item An up to date comparison of container and native computing environments using recent software and generic workloads that measure CPU, memory, and disk I/O performance
\end{itemize}

\section{Overview of Docker}

Docker is a platform that allows developers to "build, ship and run any app, anywhere". It is now the standard solution to software deployment, which is a costly and fault prone process. One way to understand Docker is through a metaphor of shipping physical goods \cite{miell}. Before containers for shipping goods were invented, special handling was applied to different types of goods. Moreover, goods were shipped from sea to air, road to sea, sea to railroad, railroad to sea, and more combinations. For each of those cases, goods were handled differently. As a result, shipping was an expensive and difficult problem. To solve the complex problem of shipping goods cheaply to all locations, shipping containers were invented. These standardized containers had specific dimensions and weight. Ships and trucks were designed to carry, load, and unload these predictably shaped items. Different types of goods are packaged inside a container, and it doesn’t matter to the carrier what types of goods are in the container. Docker does the same thing for software rather than physical goods. Docker packages software dependencies and libraries in a standard format so that they are deployable anywhere. 

\subsection{How Docker is Used}
Due to the lightweight nature of Docker containers, there are many versatile uses for them. These include prototyping software, modelling networks, and implementing a microservices architecture. Docker allows developers to experiment with new software setups without the hassle of starting a virtual machine. Since Docker provides a sandboxed environment, existing software installed on a development machine is not affected. Docker is also a portable way to package a software environment. A Docker image comes without any dependencies and runs as long as Docker is installed. Furthermore, Docker facilitates the development of a microservices architecture. One way to create a microservices architecture with Docker is to break down components into separate containers which are then composed to build a software stack \cite{miell}. 

Since a typical modern machine supports up to thousands of containers, Docker is a cheap and convenient way to model real world networks. Moreover, Docker simplifies debugging. Docker allows developers to document exactly where a bug occurs and the steps to reproduce it. An added advantage of using Docker both in development and production is that the same container that runs on a developer’s laptop runs on a server. This makes remote debugging much simpler \cite{miell}. 

\subsection{Docker Architecture}
Docker runs as multiple processes on a host machine. These processes include a client, a daemon, and a registry. The user interacts with the client to issue commands. The client sends HTTP requests to the Docker daemon, which runs as a service. The daemon in turn makes requests to other services to send and receive images. The daemon also manages the images and containers in addition to brokering interactions with registries. Docker Hub is an example of a public registry where users can share images with other users. In addition to public registries, many companies set up private registries for deploying and sharing images within the company. Docker registries are similar to source code repositories, and images can be pushed or pulled from these registries. 

Docker is built on LXC and uses kernel level primitives such as copy-on-write filesystem, cgroups, and namespaces. Docker uses Linux namespaces to give groups of processes a different view of the system. Processes within a namespace cannot access resources outside of it. Docker uses this feature to provide an isolated workspace within a container \cite{rosen}. Docker uses several types of kernel namespaces, including:
\begin{itemize}
	\item \texttt{pid} - Docker uses \texttt{pid} namespaces to separate processes within a container from processes outside it. With namespaces, processes within a namespace cannot see or influence processes outside of the namespace in other containers. 
	\item \texttt{net} – The network namespace provides containers with its own network device and virtual IP address \cite{anderson}. For example, if multiple instances of Apache were running inside a container on port 80, Apache would not be exposed outside the container unless ports were mapped. Docker by default performs a network address translation to map ports from inside the container to the host. 
	\item \texttt{mnt} – Docker allows host filesystems to be mounted in a container. This allows processes within a container to share files with the host and to have a different view of the filesystem from the host. 
\end{itemize}

Docker uses control groups, often referred to as \texttt{cgroups}, to provide resource management and accounting. For instance, users can specify the amount of RAM or number of CPU cores for a container. \cite{anderson}. Docker provides granular control to physical resources, and users can add and drop capabilities. 

Docker relies on copy-on-write union file systems to provide a copy of the file system within containers. Docker uses Linux union file systems, which work by layering on top of existing file systems. Btrfs, device mapper, and AuFS are all examples of union file systems that support copy-on-write operations. Docker containers are built on a file system layer with libraries and binaries that fake an operating system. That base file system layer is referred to as an image in Docker terminology. Changes to the file system are added as layers and can be rolled back \cite{anderson}. The user may commit these changes and push them to a registry as an image. 


\subsection{How Docker Compares to VMs}
Docker containers are quicker to spin up and deploy to machines. Since Docker uses a layered filesystem, only the changes to the filesystem rather than an entire operating system is shipped. As a result, Docker containers are lightweight to deploy and make changes to. In addition, Docker is entirely a command line utility, which makes it scriptable. Unlike starting and stopping a virtual machine through GUI management consoles, it is relatively easy to automate starting and stopping Docker containers. 

Docker containers are lightweight and more scalable than virtual machines for running multiple instances on a single physical machine. In a scalability comparison done by Ann Mary Joy \cite{joy}, containers scaled up 22 times faster than virtual machines for a load balanced WordPress application. In this experiment, the AWS EC2 auto scaling feature is used. Auto scaling is triggered when the CPU utilization goes above 80\%. Jmeter is used to send concurrent requests to WordPress. Spinning up a new container to load balance the requests took only eight seconds compared to three minutes for a new virtual machine. 

Raho et al. conducted a performance analysis comparing KVM, Xen, and Docker on ARM based devices \cite{raho}. They found a negligible performance overhead for Docker and Xen privileged domain. On the UnixBench tests, hypervisors performed better than Docker on pipe-based context switches. This performance gain was attributed to caching in hypervisors. Xen and KVM also performed better than Docker IOzone benchmarks, except for the write operation. For small packets tested with Netperf, Xen performed the best. Docker had the request/response performance. 

\section{Related Works}
Morabit et al. \cite{morabit} compared hypervisor and container performance with running applications in a native environment. They used the Y-cruncher, NBENCH, and Linpack to measure CPU performance under KVM, Docker, LXC, and OSV. For disk I/O performance, sequential reads and writes of 25 GB was tested with Bonnie++. The results show that disk I/O efficiency is still a bottleneck with KVM. LXC and Docker introduce negligible overhead, though there is a trade off in terms of security. 

Felter et al. \cite{felter} compare KVM and Docker performance with running applications in their native environment. In their benchmark, the Sysbench oltp benchmark is ran against a single instance of MySQL. The benchmark compared MySQL throughput in terms of transactions per second on Docker with an AUFS volume, Docker with normal networking using NAT, and Docker using host networking and a mounted volume, native Linux, and KVM. The results showed that MySQL transactions under KVM qcow were about 30\% compared to native. Docker volumes have noticeably better performance than files stored in AUFS. They also found that NAT introduces overheads for workloads with high packet rates. 

Xavier et al. \cite{xavier} analyzed MapReduce clusters and in HPC Environments comparing and contrasting the current container-based systems including Linux VServer, OpenVZ and Linux Containers (LXC). They evaluated the performance of MapReduce jobs on a cluster running different container systems. The results showed that all container-based systems offered a near-native performance. They also found that LXC was closest to native performance. 

\section{Methodology}
The system parameters that affect the performance of an application running in Docker are the speed of the CPU, the speed of the memory, the speed of the hard drive, the Docker syscall overhead using Linux namespaces, and the Docker control group overhead. If the application communicates over a network, the speed of the network also affects performance. Since the syscall overhead and control group overhead are not factors that I can vary, I only consider the CPU speed, memory speed, and disk I/O throughput for my experiments. The workload parameter I considered that affect the performance are 
\begin{itemize}
\item Prime number size for the sysbench CPU benchmark
\item Different operations when running STREAM to test memory performance
\item Small and large files for the file system benchmark
\item Random and sequential operations for the file system benchmark
\item Different operations for the file system benchmark, read and write
\end{itemize}
I ran a number of benchmarking applications using Docker and used the native performance as the baseline. The benchmark tools measure CPU, memory, and disk I/O performance. 

\subsection{CPU Performance}
The sysbench CPU benchmark measures the execution time for a benchmark that finds the prime numbers. The execution time is the end-to-end time from the start of the request to the end, which includes the overhead of shared memory access for the threads. The calculations are performed using 64-bit integers. Each thread executes the requests concurrently until the prime numbers up to a specified value are computed \cite{kopytov}. For this benchmark, I use two CPU workloads to represent high and low levels. For the lower CPU workload, I set the maximum prime number to 20000. For the higher CPU workload, I set the maximum prime number to 80000. 

\lstset{caption=Sysbench CPU Benchmark Source Code, label=lst:sysbenchcpu}
\begin{lstlisting}
int cpu_execute_request(sb_request_t *r, int thread_id)
{
	unsigned long long c;
	unsigned long long l;
	double t;
	unsigned long long n=0;
	log_msg_t           msg;
	log_msg_oper_t      op_msg;
	
	(void)r; /* unused */
	
	/* Prepare log message */
	msg.type = LOG_MSG_TYPE_OPER;
	msg.data = &op_msg;
	
	/* So far we're using very simple test prime number tests in 64bit */
	LOG_EVENT_START(msg, thread_id);
	
	for(c=3; c < max_prime; c++)  
	{
		t = sqrt((double)c);
		for(l = 2; l <= t; l++)
			if (c % l == 0)
				break;
		if (l > t )
			n++; 
	}
	
	LOG_EVENT_STOP(msg, thread_id);
	
	return 0;
}
\end{lstlisting}

The sysbench CPU benchmark source code is shown in listing \ref{lst:sysbenchcpu}. The event start time is logged before starting the prime calculation and the event end time is logged after the calculation. The \textit{cpu\_execute\_request} function is called 10000 times to calculate the average time it takes to compute the prime numbers. 

Fig. \ref{fig:cpu} shows the execution time of the sysbench CPU benchmark where lower is better. The performance effect of running the benchmark in Docker is negligible, at about 0.1\%. It is interesting to note that for large prime numbers, there is a performance increase when running in Docker. For small prime numbers, there is a performance decrease running inside Docker.

\subsection{Disk I/O Performance}
The sysbench also has a set of file I/O benchmarks that characterize disk performance. There are three stages for a sysbench I/O benchmark. In the \texttt{prepare} stage, sysbench creates the files. The \texttt{run} stage is where the I/O operations for the benchmark are performed. Finally, there is a \texttt{cleanup} stage where files created for the benchmark are deleted. Sysbench supports many I/O operations, including:
\begin{itemize}
	\item sequential write
	\item sequential read
	\item sequential rewrite
	\item random read
	\item random write
	\item combined random read/write
For the workloads, I have configured sysbench to create a set of files with the total size of files to 5GB. In order to clear the file system caches between each run, I unmount and format the file system. An example script for formatting a partition and mounting it in Docker to execute the random read benchmark is shown in listing \ref{lst:sysbenchio}. The accompanying Dockerfile in listing \ref{lst:dockerfile} takes the \texttt{sysbench-single.sh} script in the current directory and adds it to the working directory in the Docker image. Similarly, for measuring native I/O performance I unmount and remount a separate partition to clear the file system cache. 

\lstset{caption=Sysbench File I/O Random Read Bash Script, label=lst:sysbenchio}
\begin{lstlisting}
sudo mkfs.btrfs -f /dev/sdb1
sudo mount /dev/sda1 /mnt/benchmark/
cat > sysbench-single.sh <<EOF
#!/bin/sh
cd /benchmark
sysbench --test=fileio --file-total-size=5G prepare
sysbench --test=fileio --file-total-size=5G --file-test-mode=rndrd run >> /results/sysbench-file-randread.log
sysbench --test=fileio --file-total-size=5G cleanup
EOF
sudo docker build -t benchmark .
sudo docker run -v /results:/results -v /mnt/benchmark:/benchmark benchmark ./sysbench-single.sh
sudo docker rmi -f benchmark
sudo umount /mnt/benchmark/
\end{lstlisting}

\lstset{caption=Dockerfile for Building Benchmark Image, label=lst:dockerfile}
\begin{lstlisting}

FROM yuguang/benchmark-64bit
ADD . /scripts
WORKDIR /scripts
RUN chmod +x *.sh
\end{lstlisting}

In order to format the file system and unmount it, I ran these benchmarks on a removable USB drive. The USB has a total capacity of 16GB and a USB 3.0 interface with the test computer. 

\subsection{Memory Performance}
STREAM is a synthetic benchmark program that measures memory bandwith using simple vector kernels. It runs multiple repetitions four different kernels: \textit{copy}, \textit{scale}, \textit{add}, and \textit{triad}. STREAM uses arrays that are at least four times larger than the sum of all the last level CPU caches in order to measure the main memory bandwith. The computed result is checked as an integrity test after each run. 

\section{Discussion}
This section presents the results of the analysis. The benchmarks measure CPU, Memory, Disk I/O, and Network I/O performance \cite{bukh}. Each individual measurement is repeated 5 
times and the diagrams show the average of the measurements. 

\section{Conclusions and Future Work}

\subsection{Integration with Datamill}
Datamill workers have three standard partitions: boot, controller, and benchmark. The boot partition is for the kernel and initramfs, the controller partition holds a barebones Gentoo installation, and the controller partition is cloned to run benchmarks in the benchmark partition. As a result, the allocated disk space for the controller partition is minimal to give as much space to run experiments as possible on the benchmark partition. 

Docker stores data such as images and containers in the user’s directory on disk. If the containerized experiments were run on the controller partition, the file system on the controller partition will inevitably run out of disk space. Therefore, the proposed way to run containerized experiments is by starting the Docker daemon with an argument to specify its directory. For example to run Docker from \texttt{/mnt/benchmark}, the command to start the Docker daemon is \texttt{docker -g /mnt/benchmark -d}. This will create a set of files and folders internal to Docker in \texttt{/mnt/benchmark}. After running an experiment files and folders in \texttt{/mnt/benchmark} are deleted, removing the containers and images. 

\begin{thebibliography}{99}
	\bibitem{mccalpin}
	McCalpin, John D.: "STREAM: Sustainable Memory Bandwidth in
	High Performance Computers", a continually updated technical report
	(1991-2007), available at: "http://www.cs.virginia.edu/stream/"
	
	\bibitem{kopytov}
	Kopytov, Alexey. "SysBench manual." MySQL AB (2012).
	
	\bibitem{raho}
	Raho, Moritz, et al. "KVM, Xen and Docker: A performance analysis for ARM based NFV and cloud computing." Information, Electronic and Electrical Engineering (AIEEE), 2015 IEEE 3rd Workshop on Advances in. IEEE, 2015.
	
	\bibitem{bukh}
	Bukh, Per Nikolaj D., and Raj Jain. "The art of computer systems performance analysis, techniques for experimental design, measurement, simulation and modeling." (1992): 113-115.
	
	\bibitem{augusto}
	de Oliveira, Augusto Born, et al. "Datamill: Rigorous performance evaluation made easy." Proceedings of the 4th ACM/SPEC International Conference on Performance Engineering. ACM, 2013.
	
	\bibitem{anderson}
	Anderson, Charles. "Docker." IEEE Software 32.3 (2015).
	
	\bibitem{sysbench}
	Kopytov, Alexey. "SysBench: A System Performance Benchmark, 2004."
	
	\bibitem{miell}
	Miell, Ian, and AidanHobson Sayers. \textit{Docker in Practice}. Manning Publications, New York, USA, 2015. Print.
	
	\bibitem{merkel}
	Merkel, Dirk. "Docker: lightweight linux containers for consistent development and deployment." Linux Journal 2014.239 (2014): 2.
	
	\bibitem{joy}
	Joy, Ann Mary. "Performance comparison between linux containers and virtual machines." Computer Engineering and Applications (ICACEA), 2015 International Conference on Advances in. IEEE, 2015.
	
	\bibitem{felter}
	Felter, Wes, et al. "An updated performance comparison of virtual machines and linux containers." Performance Analysis of Systems and Software (ISPASS), 2015 IEEE International Symposium On. IEEE, 2015.
	
	\bibitem{morabit}
	Morabito, Roberto, Jimmy Kjällman, and Miika Komu. "Hypervisors vs. lightweight virtualization: a performance comparison." Cloud Engineering (IC2E), 2015 IEEE International Conference on. IEEE, 2015.
	
	\bibitem{xavier}
	Xavier, Miguel Gomes, Marcelo Veiga Neves, and Cesar Augusto Fonticielha De Rose. "A performance comparison of container-based virtualization systems for mapreduce clusters." 2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing. IEEE, 2014.
	
	\bibitem{rosen}
	Rosen, Rami. \textit{Linux Kernel Networking}. Implementation and Theory. Springer Science and Business Media New York, New York, USA, 2013
	 
\end{thebibliography}

\end{document}